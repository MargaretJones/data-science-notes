---
title: "Udemy Introduction to Data Science Notebook"
output:
  html_document: default
  html_notebook: default
---

This is my personal notebook from Udemy's Indroduction to Data Science course (taught by Nina Zumel and John Mount). It focuses on supervised learning methods using R.

#Section 1: Course Overview 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Load packages
library(tidyverse)
library(ggplot2)
```

My notes on this section are pretty sparse since I'm already familiar with R, but I walked through their example that uses home price data from amstat since that looks like it will be a good dataset to use for practice later. Here, we are pulling in a tab-seperated file of home prices and key facts related to those homes straight from its URL. We also rename the columns to make them more useful.   

```{r}
#Pull homeprice data straight from the amstat website
URL <- "https://ww2.amstat.org/publications/jse/datasets/homes76.dat.txt"
#Documentation on this dataset: http://ww2.amstat.org/publications/jse/v16n2/datasets.pardoe.html

priceData <- read.table(URL, header = TRUE , sep = "\t" , stringsAsFactors = TRUE)

#Rename columns
colnames(priceData) <- c("id","Price","Size","Lot","Bed","Bath","BathBed","Year","Age","Agesq","Garage","Status","Active","Elem","Edison Elem","Harris Elem","Adams Elem","Crest Elem","Parker Elem")
```

I also created a couple of basic plots to answer some of the quick questions I had right off the bat. I may come back and pretty these up later if I'm feeling plotty. 

How many homes are near each elementary school? 

```{r, echo = FALSE}
ggplot(priceData,aes(Elem)) + geom_bar(stat = "count")
```

What's the general distrubtution of home prices? 

```{r, echo = FALSE}
ggplot(priceData) + geom_density(aes(x = Price))
```


#Section 2: Modeling and Machine Learning 

##Validating Models Overview

Always remember to validate your model on data that it hasn't yet seen. If you want it to be predictive, you have to evaluate its performance with data outside the training set. Possible issues that may arise: 

* Bias
    + systematic error in certain regions of the data 
    + inductive bias - assumptions made in modeling (eg linear regression assumes linear relationship and additive relationship between variables)
    + note: there is a bias - variance tradeoff
* Variance 
    + overly sensitive to small variations in training data
    + produces very different models depending on the specific training set chosen from the same distribution of data
    + models with high variance are prone to overfit
* Overfit 
    + model is memorizing traning data rather than generalizing good patterns 
    + you know your model is overfit if it performs well in traning, but poorly with test data 
  
Split the data to avoid overfit. Your model will usually have a smaller traning error than test error. A small generalization error means your model is less likely to be overfitting. (Also, heads up cause some people refer to the test error the generalization error).

> Traning Error - Test Error = Generalization Error

Best solution for evaluation? Split the data 3 ways (if you have enough of it!):

* Training set - train models
* Calibration set - make adjustments before final model selection (set params, evaluate models) 
* Test set - evaluate final model... just one!

Look into cross validation and emperical resampling if you only have a little data. Also, often random splits will work, but some cases (eg. time series, duplicate records, multiple records from one source) require a structured test/train split. 

##Test/Train Split Example

Ok, the video walks through an example predicting baseball players' salary, but I'm going to use the homes data from Section 1 so I don't have to load in a second outside data set. (Plus it'll give me a chance to think about how to apply the concepts to similar but different data). One issue with using the home price data... we only have 76 data points, so this might be kind of crummy.

I've looked at the data a little bit, and weirdly, there doesn't seem to be a strong relationship between square foot and price or age and price. I'm going to see how a model that combines Age, Elem, Garage, and Size does at predicting home price! I'm worried about a model with so many variables & so little data overfitting, so I'll try to keep an eye out for that.    

```{r}
#Add a new column called logPrice to model on
priceData <- priceData %>%
  mutate(logPrice = log(Price))

priceData %>%
ggplot() + 
  geom_jitter(aes(x = Garage, y = logPrice, color = Elem))
```

Hold back 25% of the data for a test set.  

```{r}
#outcome you're trying to predict
outcome <- "logPrice"

#Set input variables (everything except the price, home id, and log price)
vars <- priceData %>%
  select(Size, BathBed, Age, Elem, Garage) %>%
  colnames()

#Split randomly
set.seed(42770)

nr = nrow(priceData)
#make the test/train assignments (set aside 25% of data for test)
is.test <- runif(nr) <= 0.25

#split the data
test <- priceData[is.test, ]
train <- priceData[!is.test, ]

# put the test marker back in the data for reproducability
priceData$is.test <- is.test

```


Ok, so we're split between a test set and training set. One important thing to note here is that it really matters HOW do do this split for a data set this size. That suggests to me that maybe this data set is just to small to do what I'm trying to do with it? One issue, is that the factor variable Elem has only 3 observations with the level "adams." If they all end up in the test data without training on it, that will cause an error with this message, so it's nessessary to at least ensure that doesn't happen. (Though I think there's issues with modeling a whole factor level based on just the 1-2 observations that may have ended up in the training data). Just for fun though, let's try fitting our first model! 

```{r}
fmla <- paste(outcome, "~", paste(vars, collapse="+"))

#model on the training data
model <- lm(fmla,data=train)

#check out dem stats
summary(model)
```

Hmmm, none of the variables in this set seem to matter much. Other splits have also suggested that garage size might be important, and that Edison Elementary might be have the most predictive power in terms of home price. 

```{r}
#create a prediction of price
pricePred <- predict(model, newdata = priceData)

#create a df of price, predicted price, and whether it was part of the training data 
perf <- data.frame(logPrice = priceData[[outcome]],
                   pred = pricePred, is.test=priceData$is.test)

#calc the squared error 
sqerr <- (perf$logPrice - perf$pred)^2

#training error
sqrt(mean(sqerr[!is.test]))

#test error 
sqrt(mean(sqerr[is.test]))


ggplot(perf, aes(x=pred, y=logPrice, color=is.test)) + 
  geom_point(aes(shape = is.test)) +
  geom_abline(slope = 1)

```

Mmm look at that scatter... My r-squared was 0.41. The training error was 0.162, and the test error was 0.170, so I might not be overfitting, but I also don't seem to be doing well at prediction. I knew I might have issues with a tiny dataset. Or it may be that a linear model just isn't the best for this. One issue, is that the way I split my test/train data has a large affect on which variables the model thinks are important. Sometimes it's a totally garage-size dominated model (seriously!), other times, the elementary schools or size of the house also seem important. 




